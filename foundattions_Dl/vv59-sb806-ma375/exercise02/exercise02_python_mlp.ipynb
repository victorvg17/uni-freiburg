{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJFZEFq4ueIi"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "The exercise focuses on implementing a small feedforward neural network and training it on the MNIST dataset.\n",
    "We recommend looking at Chapter 6 in the \"Deep Learning\" book.\n",
    "\n",
    "We provide code for structure and utility, you have to **fill in the TODO-gaps**.\n",
    "It might initially look like a lot of unnecessary code, but it keeps the network extensible. In the following exercises you can reuse what you've done here. Most common neural network libraries (*pytorch*, *tensorflow/keras*, …) are similarly structured, hence they will be easy to use once you've finished this notebook. As we will be using *pytorch* towards the end of the lecture, our API resembles the API of the pytorch framework.\n",
    "\n",
    "\n",
    "We will implement two different cost functions and play a bit with the value of the different hyperparameters to see how performances change as a function of those. In particular, we will focus on\n",
    "\n",
    "* Nonlinear activation functions\n",
    "* Multi-Layer Perceptrons (MLP) a.k.a. Feed-forward neural networks.\n",
    "* Quadratic Cost Function\n",
    "* Cross Entropy Cost Function\n",
    "\n",
    "Note that we'll implement all of these operations to operate on batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YIChkVlueIl"
   },
   "outputs": [],
   "source": [
    "# Some imports used in the code below\n",
    "\n",
    "from typing import Iterable, List, Optional, Tuple  # type annotations\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import scipy.optimize  # gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2w5-mnoueIu"
   },
   "source": [
    "The cell below contains the two main classes to structure your network. \n",
    "\n",
    "**Parameter** is used to represent trainable variables in the network, e.g., a layer's weights $w$. The weights themself are a *numpy array* as the parameter's `data` attribute. The associated parameter gradient (e.g. $\\frac{\\partial L}{\\partial w}$) can be stored in the `grad` attribute.\n",
    "\n",
    "\n",
    "\n",
    "**Module** is the base class for all parts of the network (activations, layers, …) and even the network itself. They all have to implement the `forward` and `backward` methods. For backpropagation activation will flow *forward* and gradient will flow *backward* through the *network graph and it's modules*. \n",
    "\n",
    "Additional module provides utility to check the correctness of implementation by approximating *backward* with [finite difference approximations](https://en.wikipedia.org/wiki/Finite_difference#Relation_with_derivatives)  of *forward*.\n",
    "\n",
    "*Note:* All modules operate on batches of samples. E.g. the input shape of `Linear.forward` is `(batch_size, feature_shape, 1)` (we will use the last dimension in future exercises). In this exercise we will not make use of gradients and the backwards method. In the next exercise we will see how one can update the weights of a neural network using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8f1UWYwFueIw"
   },
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"A trainable parameter.\n",
    "\n",
    "    This class not only stores the value of the parameter (self.data) but also tensors/\n",
    "    properties associated with it, such as the gradient (self.grad) of the current backward\n",
    "    pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, grad: Optional[np.ndarray] = None, name=None):\n",
    "        self.data = data  # type: np.ndarray\n",
    "        self.grad = grad  # type: Optional[np.ndarray]\n",
    "        self.name = name  # type: Optional[str]\n",
    "        self.state_dict = dict()  # dict to store additional, optional information\n",
    "        \n",
    "        \n",
    "class Module:\n",
    "    \"\"\"The base class all network modules must inherit from.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Cache of the input of the forward pass.\n",
    "        # We need it during the backward pass in most layers,\n",
    "        #  e.g., to compute the gradient w.r.t to the weights.\n",
    "        self.input_cache = None\n",
    "\n",
    "    def __call__(self, *args) -> np.ndarray:\n",
    "        \"\"\"Alias for forward, convenience function.\"\"\"\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, *args) -> np.ndarray:\n",
    "        \"\"\"Compute the forward pass through the module.\n",
    "\n",
    "        Args:\n",
    "           args: The inputs, e.g., the output of the previous layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the backward pass through the module.\n",
    "\n",
    "        This method computes the gradients with respect to the trainable\n",
    "        parameters and with respect to the first input.\n",
    "        If the module has trainable parameters, this method needs to update\n",
    "        the respective parameter.grad property.\n",
    "\n",
    "        Args:\n",
    "            grad: The gradient of the following layer.\n",
    "\n",
    "        Returns:\n",
    "            The gradient with respect to the first input argument. In general\n",
    "            it might be useful to return the gradients w.r.t. to all inputs, we\n",
    "            omit this here to keep things simple.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        \"\"\"Return the module parameters.\"\"\"\n",
    "        return []  # default to empty list\n",
    "\n",
    "    def check_gradients(self, input_args: Tuple[np.ndarray]):\n",
    "        \"\"\"Verify the implementation of the gradients.\n",
    "\n",
    "        This includes the gradient with respect to the input as well as the\n",
    "        gradients w.r.t. the parameters if the module contains any.\n",
    "\n",
    "        As the scipy grad check only works on scalar functions, we compute\n",
    "        the sum over the output to obtain a scalar.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_args, tuple), (\n",
    "            \"input_args must be a tuple but is {}\".format(type(input_args)))\n",
    "        TOLERANCE = 1e-6\n",
    "        self.check_gradients_wrt_input(input_args, TOLERANCE)\n",
    "        self.check_gradients_wrt_params(input_args, TOLERANCE)\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        \"\"\"(Re-) intialize the param's grads to 0. Helper for grad checking.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def check_gradients_wrt_input(self, input_args: Tuple[np.ndarray],\n",
    "                                  tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. input.\"\"\"\n",
    "\n",
    "        def output_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.forward for scipy.optimize.check_grad.\"\"\"\n",
    "            # we only compute the gradient w.r.t. to the first input arg.\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            return np.sum(self.forward(*args))\n",
    "\n",
    "        def grad_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.backward for scipy.optimize.check_grad.\"\"\"\n",
    "            self._zero_grad()\n",
    "            # run self.forward to store the new input\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            out = self.forward(*args)\n",
    "            # compute the gradient w.r.t. to the input\n",
    "            return np.ravel(self.backward(np.ones_like(out)))\n",
    "\n",
    "        error = scipy.optimize.check_grad(\n",
    "            output_given_input, grad_given_input, np.ravel(input_args[0]))\n",
    "        num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "        if np.squeeze(error) / num_outputs > tolerance:\n",
    "            raise RuntimeError(\"Check of gradient w.r.t. to input for {} failed.\"\n",
    "                               \"Error {:.4E} > {:.4E}.\"\n",
    "                               .format(self, np.squeeze(error), tolerance))\n",
    "\n",
    "    def check_gradients_wrt_params(self, input_args: Tuple[np.ndarray],\n",
    "                                   tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. params.\"\"\"\n",
    "        for param in self.parameters():\n",
    "            def output_given_params(new_param: np.ndarray):\n",
    "                \"\"\"Wrap self.forward, change the parameters to new_param.\"\"\"\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                return np.sum(self.forward(*input_args))\n",
    "\n",
    "            def grad_given_params(new_param: np.ndarray):\n",
    "                self._zero_grad()\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                out = self.forward(*input_args)\n",
    "                # compute the gradient w.r.t. to param\n",
    "                self.backward(np.ones_like(out))\n",
    "                return np.ravel(param.grad)\n",
    "            # flatten the param as scipy can only handle 1D params\n",
    "            param_init = np.ravel(np.copy(param.data))\n",
    "            error = scipy.optimize.check_grad(output_given_params,\n",
    "                                              grad_given_params,\n",
    "                                              param_init)\n",
    "            num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "            if np.squeeze(error) / num_outputs > tolerance:\n",
    "                raise RuntimeError(\"Check of gradient w.r.t. to param '{}' for\"\n",
    "                                   \"{} failed. Error {:.4E} > {:.4E}.\"\n",
    "                                   .format(param.name, self, error, tolerance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pvmc_mniueI2"
   },
   "source": [
    "# Nonlinearities\n",
    "\n",
    "## Sigmoid (1 point)\n",
    "\n",
    "Implement your first network module: The sigmoid activation function.\n",
    "\n",
    "Verify your sigmoid function by plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLb-27dFueI4"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def _sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "#         raise NotImplementedError\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "        # END TODO###################\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        assert len(z.shape) == 3, (\"z.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(z.shape))\n",
    "        h = self._sigmoid(z)\n",
    "        # here it's useful to store the activation \n",
    "        #  instead of the input\n",
    "        self.input_cache = h\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2iz_xG1ueI9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-4, +4, 100)\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "y = np.ravel(sigmoid(x.reshape(-1, 1, 1)))\n",
    "# equal to call of sigmoid.forward(x)\n",
    "    \n",
    "plt.plot(x,y)\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(-4, +4, 100)\n",
    "t = x.reshape(-1, 1, 1)\n",
    "t.shape\n",
    "t_zeros = np.zeros(t.shape)\n",
    "t_zeros.shape\n",
    "t2 = np.array([[1, 2, 4],\n",
    "              [14, 255, 47]])\n",
    "t3 = t2.ravel()\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMQ5DmToueJC"
   },
   "source": [
    "## Relu (1 + 0.5 points)\n",
    "\n",
    "Implement Relu and plot for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBYjj761ueJD"
   },
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = z\n",
    "        # START TODO ################\n",
    "#         raise NotImplementedError\n",
    "        z_zeros = np.zeros(z.shape)\n",
    "        return np.maximum(z, z_zeros)\n",
    "        \n",
    "        # END TODO###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdQtZ9SbueJI"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXhUhdn+8e8DsgdEWSIIEiygUpQlFHB7S9RaVAq+VuterVUUca3Wvdjqr1WrtbaK2rfV1pYlCFpLEetGKC4VJQHCJqsIEZRNlrAneX5/zNCmMSGTkJNzZub+XNdcnn1uRphnzvYcc3dERCR9NQg7gIiIhEuFQEQkzakQiIikORUCEZE0p0IgIpLmDgk7QE21bdvWs7KyarXujh07aNGiRd0GqgNRzQXRzaZcNaNcNZOKufLz8ze6e7tKZ7p7Ur2ys7O9tvLy8mq9bpCimss9utmUq2aUq2ZSMRcw26v4XtWhIRGRNKdCICKS5lQIRETSnAqBiEiaUyEQEUlzgRcCM2toZnPMbGol85qY2UQzW25ms8wsK+g8IiLy3+pjj+BmYHEV834IfOnu3YBfA4/UQx4RESkn0EJgZp2Ac4A/VLHIcOCF+PBk4HQzsyAziYgko9+8tYxPt5UGsm3zAJ9HYGaTgYeAlsDt7j60wvwFwBB3L4qPrwAGuvvGCsuNAEYAZGZmZufm5tYqT3FxMRkZGbVaN0hRzQXRzaZcNaNcNRO1XO99to/fz9/Ltzs7F3+9drlycnLy3b1/pTOrutPsYF/AUODp+PBgYGolyywEOpUbXwG0OdB2dWdx/YpqNuWqGeWqmSjlWrxuqx9z3zS/8Hfv+1tvT6/1dgjpzuKTgWFmtgrIBU4zs7EVlikCOgOY2SHAocDmADOJiCSN7bv3MXJsAa2aNuK3F/elYYNgjpwHVgjc/W537+TuWcBFwHR3v6zCYlOAK+LD58eX0bMzRSTtuTt3TC5k9eadPHVJP9q3bBrYe9X7fQRm9oCZDYuPPge0MbPlwI+Au+o7j4hIFD337ie8tuBz7hxyDAO6Hh7oe9VLG2p3nwHMiA+PLjd9N3BBfWQQEUkWH63azEOvfcyZPTO55tSjA38/3VksIhIhG7bvYdS4Ajof1ozHvteb+riiPukeTCMikqpKSsu4acIctu3exwtXDaBV00b18r4qBCIiEfGrN5fyr5WbeOyC3hzXoVW9va8ODYmIRMCbi77gmRkruHhAZ87P7lSv761CICISstWbdvKjF+fS68hW3P+dr9f7+6sQiIiEaPe+UkaOy8eAZy7NpmmjhvWeQecIRERC9NMpC1m4dhvPXdGfzoc3DyWD9ghEREIyafYacj9aw/WDv8bpx2WGlkOFQEQkBIvWbuO+VxZw4tFt+NG3eoSaRYVARKSebd21j5Hj8jm0WayZ3CENw/0q1jkCEZF65O78eNI8ir7cRe6IQbRr2STsSNojEBGpT79/ZyVvLPqCu886lm9kBdtMLlEqBCIi9WTWyk088o8lnNXrCH54Stew4/ybCoGISD1Yv303N06Yw1GHN+eX559QL83kEqVCICISsJLSMm4cH2sm98xl/WhZT83kEqWTxSIiAXvsjaXM+mQzj3+vN8ceUX/N5BIV2B6BmTU1sw/NbJ6ZLTSzn1WyzJVmtsHM5sZfVweVR0QkDG8s/Jxn/7mCSwYexXn96reZXKKC3CPYA5zm7sVm1gh418xec/cPKiw30d1vCDCHiEgoPt20g9smzeP4Iw9l9NCeYcepUmCFIP4Q+uL4aKP4Sw+mF5G0sHtfKdeNLaCBGU9f2i+UZnKJstj3dUAbN2sI5APdgDHufmeF+VcCDwEbgKXAre6+ppLtjABGAGRmZmbn5ubWKk9xcTEZGRm1WjdIUc0F0c2mXDWjXDVTF7mem7+Hdz4r4ZZ+TejTvm5+cx9MrpycnHx371/pTHcP/AW0BvKAXhWmtwGaxIevA6ZXt63s7Gyvrby8vFqvG6So5nKPbjblqhnlqpmDzTXxw9Xe5c6p/ug/Pq6bQHEHkwuY7VV8r9bL5aPuvgWYAQypMH2Tu++Jj/4eyK6PPCIiQVm4dis/+dsCTu7WhltDbiaXqCCvGmpnZq3jw82AM4CPKyzTodzoMGBxUHlERIK2ddc+Ro4t4LDmjfnNRX1p2CA6N40dSJBXDXUAXoifJ2gAvOjuU83sAWK7KFOAm8xsGFACbAauDDCPiEhg3J3bJ81j7ZZdTLx2EG0zwm8ml6ggrxoqBPpWMn10ueG7gbuDyiAiUl9+N3Mlby76gp8M7Ul2l2g0k0uUWkyIiBykD1Zu4tHXl3DO8R246uSssOPUmAqBiMhBWL9tNzeMn0OXNs15JGLN5BKlXkMiIrVUUlrGDRPmsGNPCeOvGUhGk+T8Sk3O1CIiEfDo60v48JPNPHFhH3pktgw7Tq3p0JCISC28vvBzfjdzJZcNOopz+x4ZdpyDokIgIlJDqzbu4PYX59G706H8JMLN5BKlQiAiUgO79pZy3dh8GjY0xlzajyaHRLeZXKJ0jkBEJEHuzk/+toAlX2znj1d+g06HNQ87Up3QHoGISIImfrSGyflF3HhadwYf0z7sOHVGhUBEJAELPtvK6CkLObV7W24+vXvYceqUCoGISDW27tzHyHH5tGmRXM3kEqVzBCIiB1BW5tw2aS6fb93NxGtP5PAWjcOOVOe0RyAicgDPzlzBW4vXc+/Zx9HvqMPCjhMIFQIRkSq8v2Ijj72+hKEndOCKk7LCjhMYFQIRkUp8vnU3N02YQ9e2LXjku8nZTC5ROkcgIlJBSZlzw/gCdu4tZcI1g2iRpM3kEhXkoyqbmtmHZjbPzBaa2c8qWaaJmU00s+VmNsvMsoLKIyKSqElL9zL70y956Lzj6Z7EzeQSFeShoT3Aae7eG+gDDDGzQRWW+SHwpbt3A34NPBJgHhGRar02fx2vryrh+yd2YXif5G4ml6jACoHHFMdHG8VfXmGx4cAL8eHJwOmWygfiRCTSVm4o5seTCzn60Abce85xYcepN+Ze8bu5Djcee3B9PtANGOPud1aYvwAY4u5F8fEVwEB331hhuRHACIDMzMzs3NzcWuUpLi4mIyOjVusGKaq5ILrZlKtmlKt6e0qcBz/YxZY9zh29naPaRiNXeQfzeeXk5OS7e/9KZ7p74C+gNZAH9KowfSHQqdz4CqDNgbaVnZ3ttZWXl1frdYMU1Vzu0c2mXDWjXAdWVlbmt+bO8ay7pvqMJesjk6uig8kFzPYqvlfr5fJRd98CzACGVJhVBHQGMLNDgEOBzfWRSURkv/EfrublOZ9x8+nd+WaPdmHHqXdBXjXUzsxax4ebAWcAH1dYbApwRXz4fGB6vHKJiNSLwqIt/GzKIv6nRztuOi21msklKsiLYzsAL8TPEzQAXnT3qWb2ALFdlCnAc8BfzGw5sT2BiwLMIyLyX7bs3MvIsQW0zWjMExf2oUGKNZNLVGCFwN0Lgb6VTB9dbng3cEFQGUREqlJW5vzoxXms376bF1O0mVyi1GJCRNLS0zOWM/3j9dx3Tk/6pmgzuUSpEIhI2nlv+UYef3Mpw3p35Psndgk7TuhUCEQkrexvJnd0uwweOu/4lG4ml6jU7qQkIlLOvtIyRo0vYNe+UiZe1i/lm8klSp+CiKSNh6Z9TP6nX/Lbi/vSrX3qN5NLlA4NiUhaeLVwHc+/9wlXnpTFsN4dw44TKSoEIpLyVmwo5o7J8+h7VGvuOTt9msklSoVARFLazr0ljBybT5NGDRlzST8aH6KvvYp0jkBEUpa7c8/L81m2vpg/XzWAjq2bhR0pklQaRSRljZ21mlfmruXWM3pwavf0ayaXKBUCEUlJ89Zs4cG/L+KbPdpxQ063sONEmgqBiKScL3fs5fpxBbRr2SStm8klSucIRCSllJU5t744l/XbdzPpupM4LI2bySVKewQiklKeylvOjCUbGD20J306tw47TlJQIRCRlPHOsg38+q2lnNunI5cNUjO5RAX5hLLOZpZnZovNbKGZ3VzJMoPNbKuZzY2/Rle2LRGR6qzbuoubc+fSvX0Gv1AzuRoJ8hxBCXCbuxeYWUsg38zedPdFFZZ7x92HBphDRFLc3pIyrh9XwJ59pTx9aTbNG+v0Z00Etkfg7uvcvSA+vB1YDBwZ1PuJSPr6xbTFzFm9hV+e35tu7TPCjpN0rD6eFW9mWcBMoJe7bys3fTDwElAErAVud/eFlaw/AhgBkJmZmZ2bm1urHMXFxWRkRO8vSVRzQXSzKVfNpHKuWetKeGbeHs7scgiXHNckMrmCcDC5cnJy8t29f6Uz3T3QF5AB5APnVTKvFZARHz4bWFbd9rKzs7228vLyar1ukKKayz262ZSrZlI117IvtnvPn7zm5z39nu8tKa2bUJ6anxcw26v4Xg30qiEza0TsF/84d3+5kiK0zd2L48PTgEZm1jbITCKSGnbsiTWTaxpvJteooS6CrK0grxoy4Dlgsbs/XsUyR8SXw8wGxPNsCiqTiKQGd+ful+ezYkMxv724L0cc2jTsSEktyFPrJwOXA/PNbG582j3AUQDu/ixwPjDSzEqAXcBF8V0YEZEq/eWDT5kyby23n9mDk7vpIMLBCqwQuPu7wAEv5HX3p4CngsogIqln7potPDh1Eacd257rB6uZXF3QQTURSRqbd+zl+rH5tG/ZlMe/11vN5OqI7roQkaRQWubcMnEuG4v3MnnkibRurmZydUV7BCKSFJ6cvoyZSzdw/7CenNBJzeTqkgqBiETezKUb+M3byziv35FcMuCosOOkHBUCEYm0z7bs4ubcOfRo35Kfn6tmckFIqBBU0Tn0K9NEROrS3pIyRo0rYF+p88xl/WjWuGHYkVJSonsEV1Qy7co6zCEi8hU/f3URc9ds4bELTuDodtHr/ZMqDnjVkJldDFwCdDWzKeVmtUR3AItIgKbMW8sL//qUq0/pypBeHcKOk9Kqu3z0fWAd0Bb4Vbnp24HCoEKJSHpb9sV27nqpkP5dDuPOs44NO07KO2AhcPdPgU+BE+snjoiku+I9JVw3Np/mjRsy5lI1k6sPCd1QZmbbgf09gBoDjYAd7t4qqGAikn7cnbteKuSTjTsYe/VAMlupmVx9SKgQuHvL8uNmdi4wIJBEIpK2Xnh/FVML1/Hjbx/DSV9TM7n6Uqt9Lnd/BTitjrOISBorWP0lP5+2mNOPbc/Ib34t7DhpJdFDQ+eVG20A9Oc/h4pERA7KpuI9jBpXwBGHNuXx7/VRM7l6lmjTue+UGy4BVgHD6zyNiKSd/c3kNu3Yy8sjT+LQ5o3CjpR2Ej1H8IOgg4hIevrN28t4Z9lGHjrveHodeWjYcdJSoi0mjjazv5vZBjNbb2Z/M7Ojq1mns5nlmdliM1tYRZsKM7PfmtlyMys0s361/YOISPIp3FDCk9OX8d1+nbjoG53DjpO2Ej1ZPB54EegAdAQmAROqWacEuM3djwMGAaPMrGeFZc4CusdfI4BnEswjIkmu6Mud/K5wD8dktuT/ndtLzeRClGghMHf/i7uXxF9jqeZksbuvc/eC+PB2YDFwZIXFhgN/9pgPgNZmpnvJRVLcnpJSRo0roMzh2cuy1UwuZJbIs+LN7GFgC5BLrABcCDQBxgC4++Zq1s8CZgK93H1buelTgYfjzzfGzN4G7nT32RXWH0Fsj4HMzMzs3NzcxP50FRQXF5OREb3GVVHNBdHNplw1E7Vcf160h+mrS7jmWOfkrOjk2i9qn9d+B5MrJycn3937VzrT3at9AZ8c4LWymnUzgHzgvErmvQqcUm78bSD7QNvLzs722srLy6v1ukGKai736GZTrpqJUq5X5hR5lzun+s9fXRSpXOWlYi5gtlfxvZro5aPHufvu8hPMrGnFaRWZWSPgJWCcu79cySJFQPkzRJ2AtQlmEpEks/SL7dz10nwGZB3OHd8+hnff+SLsSELi5wjeT3Dav1nszM9zwGJ3f7yKxaYA349fPTQI2Oru6xLMJCJJZH8zuRZNDuGpS/pyiJrJRUZ1zyM4gtgJ3mZm1hfYf1q/FdC8mm2fDFwOzDezufFp9wBHAbj7s8A04GxgObAT0P0KIinI3bnzpUI+3bSTcVcPpL2ayUVKdYeGvk3sSWSdgPK/6rcT+1KvksdOAB/werD4catR1aYUkaT2x/dW8WrhOu4661gGHd0m7DhSQXXPI3gBeMHMvuvuL9VTJhFJIfmfbuYX0xbzrZ6ZXPs/B7wPVUKS6MniXmb29YoT3f2BOs4jIikk1kxuDh1bN+OxC3rrprGISrQQFJcbbgoMJXaDmIhIpUrLnJtz57J5Z7yZXDM1k4uqRJvOlX9eMWb2GLErfkREKvXEW0t5d/lGHvmumslFXW2v32oO6GCfiFQq7+P1PDl9Od/r34kLv3FU2HGkGok+mGY+/+kt1ABoDzwYVCgRSV5rNu/klolz6dmhFQ8M7xV2HElAoucIhgKHAacCrYFp7p4fWCoRSUp7SkoZNb6AMneeuawfTRupmVwySPTQ0HDgL0BboBHwRzO7MbBUIpKUHvj7IgqLtvKrC3rTpU2LsONIghLdI7gaGOTuOwDM7BHgX8CTQQUTkeTy1zlFjJu1mmu/eTRnfv2IsONIDST8PAKgtNx4KdXcNSwi6WPJ59u5++X5DOx6OD8+85iw40gNJbpH8Edglpn9NT5+LrGGciKS5rbv3sfIsfm0bNqIJ9VMLikleh/B42Y2AziF2J7AD9x9TpDBRCT63J07Jhfy6eadjL96IO1bqplcMkp0jwCPPXayIMAsIpJknnv3E15b8Dl3n3UsA9VMLmlpH05EamX2qs08/NrHnNkzkxFqJpfUVAhEpMY2Fu9h1PgCjjysGY+qmVzSS/jQkIgIxJrJ3TRhDlt27uOv1w9QM7kUENgegZk9b2brzWxBFfMHm9lWM5sbf40OKouI1J3H31zC+ys28eC5vejZsVXYcaQOBLlH8CfgKeDPB1jmHXcfGmAGEalDby/+gjF5K7joG535Xv/OYceROhLYHoG7zwQ2B7V9Ealfazbv5NaJc/l6x1b8dNhXnlMlScxijw0OaONmWcBUd/9KC0IzGwy8BBQBa4Hb3X1hFdsZAYwAyMzMzM7Nza1VnuLiYjIyMmq1bpCimguim025auZgc+0tdX4+azfrd5bxs5Oa0b553fyGTNXPKygHkysnJyff3ftXOtPdA3sBWcCCKua1AjLiw2cDyxLZZnZ2ttdWXl5erdcNUlRzuUc3m3LVzMHmuuulQu9y51R/c+HndRMoLlU/r6AcTC5gtlfxvRra5aPuvs3di+PD04BGZtY2rDwiUrmX8ouY8OFqRg7+Gmf0zAw7jgQgtEJgZkdY/OJjMxsQz7IprDwi8lUff76Ne1+Zz4lHt+G2b/UIO44EJLCrhsxsAjAYaGtmRcD9xJ5lgLs/C5wPjDSzEmAXcFF890VEImDb7n2MHFtAq6aN+O3FaiaXygIrBO5+cTXznyJ2eamIRIy7c8ekQlZv3smEawbRrmWTsCNJgFTiReQr/vDOJ/xj4efcNeRYBnQ9POw4EjAVAhH5Lx+t2szD//iYIV8/gqtP7Rp2HKkHKgQi8m/rt+9m1LgCOh/WjF9ecIKayaUJNZ0TEQBKSsu4acIctu3exwtXDaBVUzWTSxcqBCICwK/eXMoHKzfz2AW9Oa6DmsmlEx0aEhHeWvQFz8xYwcUDjuL87E5hx5F6pkIgkuZWb9rJrS/OpdeRrbj/Oz3DjiMhUCEQSWO795Uyclw+Dcx45tJsmjZqGHYkCYHOEYiksZ9OWcjCtdt4/sr+dD68edhxJCTaIxBJU5NmryH3ozWMyvkapx2rZnLpTIVAJA0tWruN+15ZwElfa8OPvnVM2HEkZCoEImlm6659jByXT+vmsWZyDRvoprF0p3MEImnE3fnxpHl89uUuckcMom2GmsmJ9ghE0sr/zVzJG4u+4K6zjqV/lprJSYwKgUiamLVyE798fQln9TqCH56iZnLyH4EVAjN73szWm9mCKuabmf3WzJabWaGZ9Qsqi0i627K7jBsmzKHL4c355flqJif/Lcg9gj8BQw4w/yyge/w1AngmwCwiaauktIxn5u1h++59PH1ZP1qqmZxUEFghcPeZwOYDLDIc+LPHfAC0NrMOQeURSVePvrGEJV+W8Yv/PZ5jj1AzOfkqC/IxwWaWBUx1916VzJsKPOzu78bH3wbudPfZlSw7gtheA5mZmdm5ubm1ylNcXExGRkat1g1SVHNBdLMpV2LyvyjhyTl7OPkI55o+0cm1X9Q+r/1SMVdOTk6+u/evdKa7B/YCsoAFVcx7FTil3PjbQHZ128zOzvbaysvLq/W6QYpqLvfoZlOu6n2yodh7jf6Hf+fJd/yNt6eHHadSUfq8ykvFXMBsr+J7NcyrhoqAzuXGOwFrQ8oiklJizeQKaNDAGHNJPxrppjE5gDALwRTg+/GrhwYBW919XYh5RFLGT15ZwOJ123jiwj5qJifVCuzOYjObAAwG2ppZEXA/0AjA3Z8FpgFnA8uBncAPgsoikk4mfrSaSflF3HhaN3KObR92HEkCgRUCd7+4mvkOjArq/UXS0YLPtvKTvy3klG5tueWMHmHHkSShO4tFUsTWnfu4flwBhzdvzG8u6qNmcpIwNZ0TSQFlZc5tk+aydssuJl57Im3UTE5qQHsEIing2ZkreGvxeu45+ziyuxwWdhxJMioEIknuXys28djrSzjnhA784OSssONIElIhEEli67ft5sYJc8hq24JHvqtmclI7OkcgkqT2lZZxw/g57NhTwvhrBpLRRP+cpXb0N0ckST36+hI+XLWZJy7sQ4/MlmHHkSSmQ0MiSegfC9bxfzNXcvmgLpzb98iw40iSUyEQSTKfbNzBjycV0rtza+4belzYcSQFqBCIJJFde0sZOTafhg2NMZf0pckhDcOOJClA5whEkoS7c98rC1jyxXb+eOU36HSYmslJ3dAegUiSyP1oDS8VFHHjad0ZfIyayUndUSEQSQILPtvK/VMWcmr3ttx8evew40iKUSEQibitO/cxclw+bVo05jcX9VUzOalzOkcgEmFlZc6tL87l8627mXjtiRzeonHYkSQFaY9AJMKe+ecKpn+8nnvPPo5+R6mZnAQj0EJgZkPMbImZLTezuyqZf6WZbTCzufHX1UHmEUkm7y3fyK/eWMJ3enfkipOywo4jKSzIR1U2BMYA3yL2oPqPzGyKuy+qsOhEd78hqBwiyejzrbu5OXcOXdu24OHzjlczOQlUkHsEA4Dl7r7S3fcCucDwAN9PJCXEmskVsHNvKc9elk0LNZOTgFns0cEBbNjsfGCIu18dH78cGFj+17+ZXQk8BGwAlgK3uvuaSrY1AhgBkJmZmZ2bm1urTMXFxWRkZNRq3SBFNRdEN1sq55qweA+vf1rCdSc0YVDHuikCqfx5BSEVc+Xk5OS7e/9KZ7p7IC/gAuAP5cYvB56ssEwboEl8+DpgenXbzc7O9trKy8ur9bpBimou9+hmS9Vcrxau9S53TvXRr8yvm0Bxqfp5BSUVcwGzvYrv1SAPDRUBncuNdwLWVihCm9x9T3z090B2gHlEIm3lhmLumFxIn86tufecnmHHkTQSZCH4COhuZl3NrDFwETCl/AJm1qHc6DBgcYB5RCJr594SRo4toFFD4+lL+9H4EF3ZLfUnsLNQ7l5iZjcArwMNgefdfaGZPUBsF2UKcJOZDQNKgM3AlUHlEYkqd+e+vy5g6frt/PmqAXRs3SzsSJJmAr0cwd2nAdMqTBtdbvhu4O4gM4hE3fgPV/PynM+49YwenNq9XdhxJA1p/1MkRIVFW/jZlEV8s0c7bjytW9hxJE2pEIiEZMvOvYwcW0C7lk144sI+NFAzOQmJ7lQRCUFZmXPLxLms376bSdedxGFqJich0h6BSAjG5C1nxpINjB7akz6dW4cdR9KcCoFIPXt32UYef2spw/t05LJBXcKOI6JCIFKf1m3dxU25c+jWLoNf/K+ayUk0qBCI1JO9JWWMGlfAnn2lPKNmchIh+psoUk8eem0xBau38NQlfenWPnoNzSR9aY9ApB68WriOP763iitPymLoCR3DjiPyX1QIRAK2fH0xd0yeR7+jWnPP2ceFHUfkK1QIRAK0c28J14/Lp0mjhoxRMzmJKJ0jEAmIu3PPy/NZtr6Yv1w1kA6HqpmcRJN+nogEZOys1bwydy0/OqMHp3RvG3YckSqpEIgEYN6aLTz490UMPqYdo3LUTE6iTYVApI59uWMv149TMzlJHjpHIFKHyjzWTG7D9j1MHnkirZurmZxEX6B7BGY2xMyWmNlyM7urkvlNzGxifP4sM8sKMo9IkHbsKeFPC/fyz6UbGP2dnpzQSc3kJDkEVgjMrCEwBjgL6AlcbGYVn8j9Q+BLd+8G/Bp4JKg8IkF6Z9kGvv3ETGYWlXDtN4/m0oFHhR1JJGFBHhoaACx395UAZpYLDAcWlVtmOPDT+PBk4CkzM3f3ug7zz6UbuOfdnbQo+Gddb/qg7dgZzVwQ3WxRylXqzsoNOzi6bQvuGdiUEWfppjFJLkEWgiOBNeXGi4CBVS0Tf9j9VqANsLH8QmY2AhgBkJmZyYwZM2ocZvmXpWQ2KaOh7arxukHLiGguiG62SOUy6N2tEWd1dfbu2lWrv59BKy4uVq4aSLtc7h7IC7gA+EO58cuBJysssxDoVG58BdDmQNvNzs722srLy6v1ukGKai736GZTrppRrppJxVzAbK/iezXIk8VFQOdy452AtVUtY2aHAIcCmwPMJCIiFQRZCD4CuptZVzNrDFwETKmwzBTgivjw+cD0eOUSEZF6Etg5Ao8d878BeB1oCDzv7gvN7AFiuyhTgOeAv5jZcmJ7AhcFlUdERCoX6A1l7j4NmFZh2uhyw7uJnUsQEZGQqMWEiEiaUyEQEUlzKgQiImlOhUBEJM1Zsl2taWYbgE9ruXpbKty1HBFRzQXRzaZcNaNcNZOKubq4e7vKZiRdITgYZjbb3fuHnaOiqOaC6GZTrppRrppJt1w6NCQikuZUCERE0ly6FYL/CztAFaKaC6KbTblqRrlqJq1ypdU5AhER+a3+YIAAAAS1SURBVKp02yMQEZEKVAhERNJc2hYCM7vdzNzM2oadBcDMHjSzQjOba2ZvmFnHsDMBmNmjZvZxPNtfzSwST2Q3swvMbKGZlZlZ6Jf5mdkQM1tiZsvN7K6w8+xnZs+b2XozWxB2lv3MrLOZ5ZnZ4vj/w5vDzgRgZk3N7EMzmxfP9bOwM5VnZg3NbI6ZTa3rbadlITCzzsC3gNVhZynnUXc/wd37AFOB0dWtUE/eBHq5+wnAUuDukPPstwA4D5gZdhAzawiMAc4CegIXm1nPcFP925+AIWGHqKAEuM3djwMGAaMi8nntAU5z995AH2CImQ0KOVN5NwOLg9hwWhYC4NfAHUBkzpS7+7Zyoy2ISDZ3f8PdS+KjHxB70lzo3H2xuy8JO0fcAGC5u690971ALjA85EwAuPtMIvbUP3df5+4F8eHtxL7cjgw3FcSf6FgcH20Uf0Xi36GZdQLOAf4QxPbTrhCY2TDgM3efF3aWiszs52a2BriU6OwRlHcV8FrYISLoSGBNufEiIvDFlgzMLAvoC8wKN0lM/PDLXGA98Ka7RyIX8ASxH69lQWw80AfThMXM3gKOqGTWvcA9wJn1myjmQLnc/W/ufi9wr5ndDdwA3B+FXPFl7iW2Sz+uPjIlmisirJJpkfglGWVmlgG8BNxSYY84NO5eCvSJnwv7q5n1cvdQz6+Y2VBgvbvnm9ngIN4jJQuBu59R2XQzOx7oCswzM4gd5igwswHu/nlYuSoxHniVeioE1eUysyuAocDp9flM6Rp8XmErAjqXG+8ErA0pS1Iws0bEisA4d3857DwVufsWM5tB7PxK2CfaTwaGmdnZQFOglZmNdffL6uoN0urQkLvPd/f27p7l7lnE/gH3q48iUB0z615udBjwcVhZyjOzIcCdwDB33xl2noj6COhuZl3NrDGxZ29PCTlTZFnsV9hzwGJ3fzzsPPuZWbv9V8WZWTPgDCLw79Dd73b3TvHvrIuA6XVZBCDNCkHEPWxmC8yskNihq0hcUgc8BbQE3oxf2vps2IEAzOx/zawIOBF41cxeDytL/GT6DcDrxE58vujuC8PKU56ZTQD+BRxjZkVm9sOwMxH7hXs5cFr879Tc+K/dsHUA8uL/Bj8ido6gzi/VjCK1mBARSXPaIxARSXMqBCIiaU6FQEQkzakQiIikORUCEZE0p0IgcgBm9n4A28wys0vqersitaVCIHIA7n5SAJvNAlQIJDJUCEQOwMyK4/8dbGYzzGxy/PkM4+J3yGJmq8zskXgv+w/NrFt8+p/M7PyK2wIeBk6N30h1a33/mUQqUiEQSVxf4BZizxw4mtgdsvttc/cBxO7EfqKa7dwFvOPufdz914EkFakBFQKRxH3o7kXuXgbMJXaIZ78J5f57Yn0HEzkYKgQiidtTbriU/+7e65UMlxD/NxY/jNQ40HQitaRCIFI3Liz333/Fh1cB2fHh4cSeeAWwnVgjP5FISMnnEYiEoImZzSL24+ri+LTfA38zsw+Bt4Ed8emFQImZzQP+pPMEEjZ1HxU5SGa2Cujv7hvDziJSGzo0JCKS5rRHICKS5rRHICKS5lQIRETSnAqBiEiaUyEQEUlzKgQiImnu/wP3XVi5svZcnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Relu\n",
    "x = np.linspace(-4.0, 4.0, 100)\n",
    "relu = Relu()\n",
    "\n",
    "# START TODO ################\n",
    "y = np.ravel(relu.forward(x.reshape(-1, 1, 1)))\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# END TODO###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zsNa01IueJN"
   },
   "source": [
    "## Softmax (1 point)\n",
    "\n",
    "Implement the numerical stable softmax. We will not need the backward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWZf9BhTueJP"
   },
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    def _softmax(self, z):\n",
    "        # don't reduce (sum) over batch axis\n",
    "        reduction_axes = tuple(range(1, len(z.shape))) \n",
    "        \n",
    "        # START TODO ################\n",
    "        dr = np.sum(np.exp(z), reduction_axes)\n",
    "        h = np.exp(z)/dr\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Shift input for numerical stability.\n",
    "        # END TODO###################\n",
    "        return h\n",
    "    \n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        h = self._softmax(z)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8wBWL-7ueJT"
   },
   "outputs": [],
   "source": [
    "# Check your softmax\n",
    "softmax = Softmax()\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]).reshape(1, -1, 1)\n",
    "\n",
    "# Testcase from https://en.wikipedia.org/wiki/Softmax_function#Example\n",
    "np.testing.assert_allclose(\n",
    "    np.ravel(softmax(x)), \n",
    "    [0.02364054, 0.06426166, 0.1746813, 0.474833, 0.02364054, 0.06426166, 0.1746813],\n",
    "    rtol=1e-5, err_msg=\"Softmax is not correct implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]).reshape(1, -1, 1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuHOIiLhueJX"
   },
   "source": [
    "# Linear Layer (2 points)\n",
    "\n",
    "Implement a linear (in other frameworks also called dense or fully connected) network layer. \n",
    "Here you also have to use the Parameter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 1) (2, 3)\n",
      "t5:  (1, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[12, 65, 53],\n",
    "             [46, 42, 88]])\n",
    "\n",
    "t3 = np.expand_dims(t, -1)\n",
    "print(t3.shape, t.shape)\n",
    "t5 = np.dot(t3.T, t)\n",
    "print(\"t5: \",t5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inuuMNmUueJY"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        w_data = 0.01 * np.random.randn(out_features, in_features)\n",
    "#         w_data = np.expand_dims(w_data, -1)\n",
    "        self.W = Parameter(w_data, \"W\")\n",
    "        \n",
    "        b_data = 0.01 * np.ones((out_features, 1))\n",
    "        self.b = Parameter(b_data, \"b\")\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 3, (\"x.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(x.shape))\n",
    "#         print(\"w_data: \",self.W.data)\n",
    "#         print(\"b_data: \",self.b.data)\n",
    "        self.input_cache = x\n",
    "        # START TODO ################\n",
    "        # Remember: Access weight data through self.W.data\n",
    "        return np.matmul(self.W.data, x) + self.b.data\n",
    "        # END TODO ##################\n",
    "         \n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # START TODO ################ \n",
    "        # Return all parameters of Linear\n",
    "#         raise NotImplementedError\n",
    "        return self.W.data, self.b.data\n",
    "        # END TODO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear class test case\n",
    "linu = Linear(2, 2)\n",
    "X = np.array([[12, 34, 54, 67],\n",
    "             [12, 34, 54, 67]]).reshape(4,2,1)\n",
    "W1 = np.array([[1, 1],\n",
    "             [1, 1]])\n",
    "c1 = np.array([0, -1]).reshape(2, 1)\n",
    "linu.W = Parameter(W1, \"weight1\")\n",
    "linu.b = Parameter(c1, \"bias1\")\n",
    "\n",
    "result = linu.forward(X)\n",
    "assert result.shape == (4, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8FtzDUfueJd"
   },
   "source": [
    "# Cost Functions (1 point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xezzvQYTueJe"
   },
   "outputs": [],
   "source": [
    "# Define the Cross-Entropy cost functions\n",
    "class CrossEntropyLoss(Module):\n",
    "    \"\"\"Compute the cross entropy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the cross entropy, mean over batch size.\"\"\"\n",
    "        a = self.softmax(a)\n",
    "        self.input_cache = a, y\n",
    "        # compute the mean over the batch\n",
    "        # START TODO ################ \n",
    "#         return NotImplementedError\n",
    "\n",
    "        N = y.shape[0]\n",
    "        print(f'y:{y}, a:{a}, np.log(a):{np.log(a)}, y * np.log(a): {y * np.log(a)}')\n",
    "        return -(1/N) * np.sum(y * np.log(a))\n",
    "        # END TODO ##################\n",
    "\n",
    "\n",
    "class MSELoss(Module):\n",
    "    \"\"\"Compute the mean squared error loss.\"\"\"\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = a, y\n",
    "        return np.sum(0.5 * np.linalg.norm(a - y, axis=-1)**2) / len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzJ8oxApueJi"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Sequential Network (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14vfdOnGueJk"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"A sequential container to stack modules.\n",
    "\n",
    "    Modules will be added to it in the order they are passed to the\n",
    "    constructor.\n",
    "\n",
    "    Example network with one hidden layer:\n",
    "    model = Sequential(\n",
    "                  Linear(5,10),\n",
    "                  ReLU(),\n",
    "                  Linear(10,10),\n",
    "                )\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: List[Module]):\n",
    "        super().__init__()\n",
    "        self.modules = args\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################ \n",
    "#         raise NotImplementedError\n",
    "        # Remember: module(x) is equivalent to module.forward(x)\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "#             print(\"module: \", module,\"value: \", x)\n",
    "        # END TODO ##################\n",
    "        return x\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # iterate over modules and retrieve their parameters, iterate over\n",
    "        # parameters to flatten the list\n",
    "        return [param for module in self.modules\n",
    "                for param in module.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmNlenRuueKF"
   },
   "source": [
    "# Experiments\n",
    "\n",
    "We will now implement a simple multilayer perceptron whose output aprroximates the output of the [XOR](https://en.wikipedia.org/wiki/XOR_gate) operation. At this point we are not concerned to generalize; we only care to learn a function that can classify each pair in $X = \\{ [0,0]^T, [0,1]^T, [1,0]^T, [1,1]^T \\}$ to one of the classes in $Y = \\{ 0, 1 \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-YiK7YfHgXl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# true labels\n",
    "Y = np.array([0, 1, 1, 0],)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4leewMl5TQA"
   },
   "source": [
    "\n",
    "1) Fit a logistic regression model to the X data. What is the predicted output and the best accuracy you can achieve? (1 point)\n",
    "\n",
    "**HINT**: *You can use* [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) *with the LBFGS solver.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCErVBlKz2By"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\" Implement a logistic regression model for binary classification.\n",
    "  Args:\n",
    "    x: np.ndarray with shape (nr_examples, nr_features). Input examples.\n",
    "    y: np.ndarray with shape (nr_examples,). True labels.\n",
    "\n",
    "  Returns:\n",
    "    Tuple[prediction, score]:\n",
    "      prediction: np.ndarray with shape (nr_examples,). Predicted labels.\n",
    "      score: float. Overall score of the model.\n",
    "  \"\"\"\n",
    "  # START TODO ################\n",
    "#     return NotImplementedError\n",
    "    clf = LogisticRegression(solver = 'lbfgs').fit(x, y)\n",
    "    prediction = clf.predict(x)\n",
    "    score = clf.score(x, y)\n",
    "    return [prediction, score]\n",
    "  # END TODO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CAxo8FSW-re6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, s = logistic_regression(X, Y)\n",
    "p\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdBi_Qcy5aVe"
   },
   "source": [
    "2) *Now define a sequential model (Linear, Relu, Linear) with 2 hidden units. Our model output will therefore be computed as:* $ f(x) = w^T max\\{ 0, W^TX + c \\} + b ,$ *where $W = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$, $c = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}$, $w = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$ and $b = 0$. For this part you have to:*\n",
    "  * instantiate the sequential model. (1 point)\n",
    "  * set the weights of the sequential model to the corresponding minimizer's values as shown above. (0.5 points)\n",
    "  * Propagate the input examples through the network to get the output. (0.5 points)\n",
    "  * Compute the cross entropy loss. (0.5 points)\n",
    "\n",
    "*NOTE: These parameters are actually the minimizers of some cost, e.g. one of the cost functions shown above. In the next exercise, we will learn how backpropagation is used to find these minimizers of the cost function using gradient descent.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0D7HRj-GueKG"
   },
   "outputs": [],
   "source": [
    "# START TODO ################ \n",
    "# Define the model here!\n",
    "linear1 = Linear(in_features=2, out_features=2)\n",
    "\n",
    "linear2 = Linear(in_features=2, out_features=1)\n",
    "\n",
    "relu = Relu()\n",
    "model = Sequential(\n",
    "                  linear1,\n",
    "                  relu,\n",
    "                  linear2\n",
    "                )\n",
    "# END TODO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E71BjGgsKJOI"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# change the model weights\n",
    "W1 = np.array([[1, 1],\n",
    "             [1, 1]])\n",
    "c1 = np.array([0, -1]).reshape(-1, 1)\n",
    "linear1.W = Parameter(W1, \"weight1\")\n",
    "linear1.b = Parameter(c1, \"bias1\")\n",
    "\n",
    "W2 = np.array([1, -2]).reshape(1, 2)\n",
    "c2 = 0.0\n",
    "linear2.W = Parameter(W2, \"weight2\")\n",
    "linear2.b = Parameter(c2, \"bias2\")\n",
    "# END TODO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1WT5izUJ0Sc"
   },
   "outputs": [],
   "source": [
    "# expand the shape of the input tensor\n",
    "x = np.expand_dims(X, -1)\n",
    "\n",
    "def predict(model: Module, input_data: np.ndarray, output_size: int=1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Propagate the input through the network and return the output.\n",
    "        Args:\n",
    "        model: Module. Sequential model representing the neural network\n",
    "        input_data: np.ndarray of shape (nr_examples, nr_features). Input data.\n",
    "        output_size: int. The output neurons in the MLP. This would be 1 for a binary classification task.\n",
    "        Returns:\n",
    "        prediction: np.ndarray of shape (nr_examples, output_size)\n",
    "    \"\"\"\n",
    "    # START TODO ################\n",
    "    # propagate the input_data through the model and reshape accordingly \n",
    "    # to (nr_examples, output_size)\n",
    "    prediction = model.forward(input_data)\n",
    "#     assert prediction.shape == (np.shape(input_data)[0], output_size)\n",
    "\n",
    "    # END TODO ##################\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjK-ui8W_5zk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(model, x)\n",
    "print( prediction.shape)\n",
    "\n",
    "assert np.array_equal(np.ravel(prediction), np.array([0, 1, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gn-OwwBiz4JE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:[0 1 1 0], a:[1. 1. 1. 1.], np.log(a):[0. 0. 0. 0.], y * np.log(a): [0. 0. 0. 0.]\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss()\n",
    "# loss = loss_fn.forward(prediction.reshape(4, 1), Y.reshape(4,1))\n",
    "loss = loss_fn.forward(prediction.flatten(), Y.flatten())\n",
    "\n",
    "# START TODO ###\n",
    "#############\n",
    "# given the true labels Y and the predictions compute the cross entropy loss defined above\n",
    "# x_test = np.array([0,1,1,0])\n",
    "# loss = loss_fn(x_test, x_test)\n",
    "# END TODO ##################\n",
    "print(loss)\n",
    "\n",
    "assert loss == -.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:[12 34 56 56 12 34 45 57], t2: [[12]\n",
      " [34]\n",
      " [56]\n",
      " [56]\n",
      " [12]\n",
      " [34]\n",
      " [45]\n",
      " [57]], t2.shape:(8, 1),t1.shape:(8,)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[12, 34, 56, 56],\n",
    "             [12, 34, 45, 57]])\n",
    "t1 = t.flatten()\n",
    "t2 = t.reshape(-1,1)\n",
    "print(f't1:{t1}, t2: {t2}, t2.shape:{t2.shape},t1.shape:{t1.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ChWTTjPNC-rS"
   },
   "source": [
    "3) Instantiate a new sequential model with 3 hidden units this time. Assign the correct values to the model parameters such that the output of the neural network remains the same as in the model with 2 hidden units. How many minimizers does the neural network have in this case? (1.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mg6inulTAxwd"
   },
   "outputs": [],
   "source": [
    "# START TODO ################ \n",
    "# Define the model here!\n",
    "\n",
    "# END TODO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XE301UaNA27R"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# change the model weights\n",
    "\n",
    "# END TODO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2WWaxzlBLBR"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_3units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ac69e3431426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_3units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_3units' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = predict(model_3units, x)\n",
    "print(prediction)\n",
    "\n",
    "assert np.array_equal(np.ravel(prediction), np.array([0, 1, 1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzidWhY_Ekqb"
   },
   "source": [
    "(BONUS): After transforming the input data into the hidden representation space, generate two plots showing the dataset in the input and representation spaces, respectively (the same as the one in Figure 6.1, [Chapter 6, Deep Learnig Book](http://www.deeplearningbook.org/contents/mlp.html)). (1.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsWaa2fnntSy"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x1: np.ndarray, x2: np.ndarray, h1: np.ndarray, h2: np.ndarray, y: np.ndarray):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x1: np.ndarray with shape (nr_examples,). First input features.\n",
    "    x2: np.ndarray with shape (nr_examples,). Second input feature.\n",
    "    h1: np.ndarray with shape (nr_examples,). First learned features.\n",
    "    h2: np.ndarray with shape (nr_examples,). Second learned feature.\n",
    "    y: np.ndarray with shape (nr_examples,). True labels.\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for k, (i, j) in enumerate(zip(x1, x2)):\n",
    "        ax[0].scatter(i, j, c='b', marker=r\"${}$\".format(y[k]), s=100)\n",
    "    for k, (i, j) in enumerate(zip(h1, h2)):\n",
    "        ax[1].scatter(i, j, c='b', marker=r\"${}$\".format(y[k]), s=100)\n",
    "\n",
    "    ax[0].set_yticks([0, 1])\n",
    "    ax[0].set_xticks([0, 1])\n",
    "    ax[0].set_xlabel('x1')\n",
    "    ax[0].set_ylabel('x2')\n",
    "#     ax[0].set_title(\"Original x space\")\n",
    "\n",
    "    ax[1].set_yticks([0, 1])\n",
    "    ax[1].set_xticks([0, 1, 2])\n",
    "    ax[1].set_xlabel('h1')\n",
    "    ax[1].set_ylabel('h2')\n",
    "    ax[1].set_title(\"Learned h space\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5kvCHpNDklm"
   },
   "outputs": [],
   "source": [
    "def extract_hidden(full_model: Module, x: np.ndarray) -> np.ndarray:\n",
    "  \"\"\" Function to extraxt the hidden representation from a MLP.\n",
    "  Args:\n",
    "    full_model: Module. The sequential model used as a classifier\n",
    "    x: np.ndarray with shape (nr_examples, nr_features). Input examples\n",
    "  Returns:\n",
    "    h: np.ndarray with shape (nr_examples, nr_features). Hidden representation of inputs.\n",
    "  \"\"\"\n",
    "  # START TODO ################ \n",
    "  # Extract the hidden features from the sequential model defined above and\n",
    "  # compute the hidden representation after propagating the input through\n",
    "  # the first Linear layer and the ReLU function.\n",
    "\n",
    "  # END TODO ##################\n",
    "\n",
    "h = extract_hidden(model, X)\n",
    "plot(X[:, 0], X[:, 1], h[:, 0], h[:, 1], Y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-atiph1GueKF"
   },
   "source": [
    "** Your feedback on exercise 2: ** \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "exercise02_python_mlp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
