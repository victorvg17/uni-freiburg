{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Implement the different exploration strategies.\n",
    "\n",
    "  * mab is a MAB (MultiArmedBandit) object as defined below\n",
    "  * epsilon is a scalar, which influences the amount of random actions\n",
    "  * schedule is a callable decaying epsilon\n",
    "\n",
    "You can get the approximated Q-values via mab.bandit_q_values and the different\n",
    "counters for the bandits via mab.bandit_counters. mab.no_actions gives you the number\n",
    "of arms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(mab, epsilon):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaying_epsilon_greedy(mab, epsilon_init, schedule):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(mab):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(mab, c):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(mab, tau):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, bias, q_value=0, counter=0):\n",
    "        self.bias = bias\n",
    "        self.q_value = q_value\n",
    "        self.counter = counter\n",
    "\n",
    "    def pull(self):\n",
    "        self.counter += 1\n",
    "        reward = np.clip(self.bias + np.random.uniform(), 0, 1)\n",
    "        self.q_value = self.q_value + 1/self.counter * (reward - self.q_value)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB:\n",
    "    def __init__(self, best_action, *bandits):\n",
    "        self.bandits = bandits\n",
    "        self._no_actions = len(bandits)\n",
    "        self.step_counter = 0\n",
    "        self.best_action = best_action\n",
    "\n",
    "    def pull(self, action):\n",
    "        self.step_counter += 1\n",
    "        return self.bandits[action].pull(), self.bandits[action].q_value\n",
    "\n",
    "    def run(self, no_rounds, exploration_strategy, **strategy_parameters):\n",
    "        regrets = []\n",
    "        rewards = []\n",
    "        for i in range(no_rounds):\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\rRound {}/{}\".format(i + 1, no_rounds), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            action = exploration_strategy(self, **strategy_parameters)\n",
    "            reward, q = self.pull(action)\n",
    "            best_action_value = self.best_action(self)[1]\n",
    "            regret = best_action_value - q\n",
    "            regrets.append(regret)\n",
    "            rewards.append(reward)\n",
    "        return regrets, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def bandit_counters(self):\n",
    "    return np.array([bandit.counter for bandit in self.bandits])\n",
    "@property\n",
    "def bandit_q_values(self):\n",
    "    return np.array([bandit.q_value for bandit in self.bandits])\n",
    "@property\n",
    "def no_actions(self):\n",
    "    return self._no_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(regrets):\n",
    "    for strategy, regret in regrets.items():\n",
    "        total_regret = np.cumsum(regret)\n",
    "        plt.ylabel('Total Regret')\n",
    "        plt.xlabel('Rounds')\n",
    "        plt.plot(np.arange(len(total_regret)), total_regret, label=strategy)\n",
    "    plt.legend()\n",
    "    plt.savefig('regret.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    no_rounds = 1000000\n",
    "    def schedule(mab, epsilon_init):\n",
    "        # raise NotImplementedError()\n",
    "        return (1 - mab.no_actions()/no_rounds) * epsilon_init\n",
    "\n",
    "    epsilon = 0.5\n",
    "    epsilon_init = 0.6\n",
    "    tau = 0.01\n",
    "    c = 1.0\n",
    "\n",
    "    strategies = {\n",
    "    epsilon_greedy: {'epsilon': epsilon},\n",
    "    decaying_epsilon_greedy: {'epsilon_init': epsilon_init, 'schedule': schedule},\n",
    "    random: {},\n",
    "    ucb: {'c': c},\n",
    "    softmax: {'tau': tau}\n",
    "    }\n",
    "\n",
    "  average_total_returns = {}\n",
    "  total_regrets = {}\n",
    "  num_actions = 10\n",
    "  biases = [1.0 / k for k in range(5, 5+num_actions)]\n",
    "  best_action_index = 0\n",
    "  best_action_value = 0.7\n",
    "  def best_action(mab):\n",
    "    return best_action_index, best_action_value\n",
    "  for strategy, parameters in strategies.items():\n",
    "    print(strategy.__name__)\n",
    "    bandits = [Bandit(bias, 1-bias) for bias in biases]\n",
    "    mab = MAB(best_action, *bandits)\n",
    "    total_regret, average_total_return = mab.run(no_rounds, strategy, **parameters)\n",
    "    print(\"\\n\")\n",
    "    average_total_returns[strategy.__name__] = average_total_return\n",
    "    total_regrets[strategy.__name__] = total_regret\n",
    "  plot(total_regrets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
