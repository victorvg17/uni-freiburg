{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Implement the different exploration strategies.\n",
    "\n",
    "  * mab is a MAB (MultiArmedBandit) object as defined below\n",
    "  * epsilon is a scalar, which influences the amount of random actions\n",
    "  * schedule is a callable decaying epsilon\n",
    "\n",
    "You can get the approximated Q-values via mab.bandit_q_values and the different\n",
    "counters for the bandits via mab.bandit_counters. mab.no_actions gives you the number\n",
    "of arms.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     actions: actions-1 to acitons-6\n",
    "#     with prop: (1-epsilon), select the greedy action: action-i\n",
    "#     with prop: epsilon, select a random action: action-1 - aciton-6\n",
    "def epsilon_greedy(mab, epsilon):\n",
    "    mab_counters = np.array(mab.bandit_counters()) #okay, all history of the action-1 - aciton-6\n",
    "    mab_q_values = mab.bandit_q_values() #after updating the q-values, I select the one action with highest\n",
    "    total_actions = mab.no_actions() #total number of arms\n",
    "    prob = mab_q_values/np.sum(mab_q_values)\n",
    "    for i in range(0, total_actions):\n",
    "        prob[i] = (1-epsilon)*prob[i] #greedy selection\n",
    "    random_arm = np.random.randint(0, total_actions, size=1)\n",
    "#     raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaying_epsilon_greedy(mab, epsilon_init, schedule):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(mab):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(mab, c):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(mab, tau):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, bias, q_value=0, counter=0):\n",
    "        self.bias = bias\n",
    "        self.q_value = q_value\n",
    "        self.counter = counter\n",
    "\n",
    "    def pull(self):\n",
    "        self.counter += 1\n",
    "        reward = np.clip(self.bias + np.random.uniform(), 0, 1)\n",
    "        self.q_value = self.q_value + 1/self.counter * (reward - self.q_value)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6588399937670025"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = np.clip(0.1 + np.random.uniform(), 0, 1)\n",
    "import matplotlib.pyplot as plt\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <function epsilon_greedy at 0x112f59400>   {'epsilon': 0.5}\n",
      "  <function decaying_epsilon_greedy at 0x112f59c80>   {'epsilon_init': 0.6}\n",
      "  <function random at 0x112f59e18>   {}\n",
      "  <function ucb at 0x112f59ea0>   {'c': 1.0}\n",
      "  <function softmax at 0x112f59f28>   {'tau': 0.01}\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "epsilon_init = 0.6\n",
    "tau = 0.01\n",
    "c = 1.0\n",
    "strategies = {\n",
    "    epsilon_greedy: {'epsilon': epsilon},\n",
    "    decaying_epsilon_greedy: {'epsilon_init': epsilon_init},\n",
    "    random: {},\n",
    "    ucb: {'c': c},\n",
    "    softmax: {'tau': tau}\n",
    "  }\n",
    "for strategy, parameters in strategies.items():\n",
    "    print(\" \",strategy, \" \", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB:\n",
    "    def __init__(self, best_action, *bandits):\n",
    "        self.bandits = bandits\n",
    "        self._no_actions = len(bandits)\n",
    "        self.step_counter = 0\n",
    "        self.best_action = best_action\n",
    "\n",
    "    def pull(self, action):\n",
    "        self.step_counter += 1\n",
    "        return self.bandits[action].pull(), self.bandits[action].q_value\n",
    "\n",
    "    def run(self, no_rounds, exploration_strategy, **strategy_parameters):\n",
    "        regrets = []\n",
    "        rewards = []\n",
    "        for i in range(no_rounds):\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\rRound {}/{}\".format(i + 1, no_rounds), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            action = exploration_strategy(self, **strategy_parameters)\n",
    "            reward, q = self.pull(action)\n",
    "            best_action_value = self.best_action(self)[1]\n",
    "            regret = best_action_value - q\n",
    "            regrets.append(regret)\n",
    "            rewards.append(reward)\n",
    "        return regrets, rewards\n",
    "\n",
    "    @property\n",
    "    def bandit_counters(self):\n",
    "        return np.array([bandit.counter for bandit in self.bandits])\n",
    "\n",
    "    @property\n",
    "    def bandit_q_values(self):\n",
    "        return np.array([bandit.q_value for bandit in self.bandits])\n",
    "\n",
    "    @property\n",
    "    def no_actions(self):\n",
    "        return self._no_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(regrets):\n",
    "    for strategy, regret in regrets.items():\n",
    "        total_regret = np.cumsum(regret)\n",
    "        plt.ylabel('Total Regret')\n",
    "        plt.xlabel('Rounds')\n",
    "        plt.plot(np.arange(len(total_regret)), total_regret, label=strategy)\n",
    "    plt.legend()\n",
    "    plt.savefig('regret.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon_greedy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a15ac99ea908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbandits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBandit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mmab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtotal_regret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_total_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0maverage_total_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_total_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-601dbd69fcbc>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, no_rounds, exploration_strategy, **strategy_parameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\rRound {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexploration_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mstrategy_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mbest_action_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b0dfe358cdc5>\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[0;34m(mab, epsilon)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     with prop: epsilon, select a random action: action-1 - aciton-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmab_counters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandit_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#okay, all history of the action-1 - aciton-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmab_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandit_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#after updating the q-values, I select the one action with highest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#total number of arms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    no_rounds = 1000000\n",
    "    def schedule(mab, epsilon_init):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    epsilon = 0.5\n",
    "    epsilon_init = 0.6\n",
    "    tau = 0.01\n",
    "    c = 1.0\n",
    "\n",
    "    strategies = {\n",
    "        epsilon_greedy: {'epsilon': epsilon},\n",
    "        decaying_epsilon_greedy: {'epsilon_init': epsilon_init, 'schedule': schedule},\n",
    "        random: {},\n",
    "        ucb: {'c': c},\n",
    "        softmax: {'tau': tau}\n",
    "    }\n",
    "\n",
    "    average_total_returns = {}\n",
    "    total_regrets = {}\n",
    "    num_actions = 10\n",
    "    biases = [1.0 / k for k in range(5, 5+num_actions)]\n",
    "    best_action_index = 0\n",
    "    best_action_value = 0.7\n",
    "    def best_action(mab):\n",
    "        return best_action_index, best_action_value\n",
    "    for strategy, parameters in strategies.items():\n",
    "        print(strategy.__name__)\n",
    "        bandits = [Bandit(bias, 1-bias) for bias in biases]\n",
    "        mab = MAB(best_action, *bandits)\n",
    "        total_regret, average_total_return = mab.run(no_rounds, strategy, **parameters)\n",
    "        print(\"\\n\")\n",
    "        average_total_returns[strategy.__name__] = average_total_return\n",
    "        total_regrets[strategy.__name__] = total_regret\n",
    "    plot(total_regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
